{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Home](http://brandonrose.org)\n",
    "\n",
    "# Entity Extraction and Network Analysis\n",
    "Or, how you can extract meaningful information from raw text and use it to analyze the networks of individuals hidden within your data set.\n",
    "<img src=\"network_diagram_example.png\" alt=\"Network Diagram\" style=\"width: 800px; display:inline\"/>\n",
    "\n",
    "We are all drowning in text. Fortunately there are a number of data science strategies for handling the deluge. If you'd like to learn about using machine learning for this check out my [guide on document clustering](http://brandonrose.org/clustering). In this guide I'm going to walk you through a strategy for making sense of massive troves of unstructured text using entity extration and network analysis. These strategies are actively employed for legal e-discovery and within law enforcement and the intelligence community. Imagine you work at the FBI and you just uncovered a massive trove of documents on a confiscated laptop or server. What would you do? This guide offers an approach for dealing with this type of scenario. By the end of it you'll have generated a graph like the one above, which you can use to analyze the network hidden within your data set.\n",
    "\n",
    "### Overview\n",
    "\n",
    "We are going take a set of documents (in our case, news articles), extract entities from within them, and develop a social network based on entity document co-occurrence. This can be a useful approach for getting a sense of which entities exist in a set of documents and how those entities might be related. I'll talk more about using document co-occurrence as the mechanism for drawing an edge in a social network graph later.\n",
    "\n",
    "In this guide I rely on 4 primary pieces of software:\n",
    "\n",
    "1. Stanford Core NLP\n",
    "2. Fuzzywuzzy\n",
    "3. Networkx\n",
    "4. D3.js\n",
    "\n",
    "If you're not familiar with these libraries, don't worry, I'll make it easy to get off to the races with them in no time.\n",
    "\n",
    "Note that my [github repo](https://github.com/brandomr/ner2sna) for the whole project is available. You can use `corpus.txt` as a sample data set if you'd like. Also, make sure to capture the `force` directory when you try to run this on your own. You need `force/force.html`, `force/force.css`, and `force/force.js` in order to create the chart at the end of the guide.\n",
    "\n",
    "If you have any questions for me, feel free to reach out on Twitter to [@brandonmrose](http://twitter.com/brandonmrose) or open up an issue on the github repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing CoreNLP with Docker\n",
    "First, we need to get Core NLP running on Docker. If you're not familiar with Docker, that's ok! It's an easy to use containerization service. The concept is that anywhere you can run docker you can run a docker container. Period. No need to worry about dependency management, just get docker running and pull down the container you need. Easy.\n",
    "\n",
    "**Stanford Core NLP** is one of the most popular natural language processing tools out there. It has a ton of functionality which includes part of speech tagging, parsing, lemmatization, tokenization, and what we are interested in: **named entity recognition (NER)**. NER is the process of analyzing text in order to find people, places, and organizations contained within the text. These named entities will form the basis of the rest of our analysis, so being able to extract them from text is critical.\n",
    "\n",
    "### Installing Docker\n",
    "Docker now has great installation instructions (trust me, this wasn't always the case). I'm using a Mac so I followed their [Mac OSX Docker installation guide](https://docs.docker.com/docker-for-mac/install/). If you're using Windows check out their [Windows install guide](https://docs.docker.com/docker-for-windows/install/). If you're using Linux I'm pretty sure you'll be able to get Docker installed on your own.\n",
    "\n",
    "To verify the installation was successful go to your command line and try running:\n",
    "\n",
    "```\n",
    "docker ps\n",
    "```\n",
    "\n",
    "You should an empty docker listing that looks like (I truncated a couple columns, but you get the idea):\n",
    "```\n",
    "CONTAINER ID        IMAGE               COMMAND             CREATED\n",
    "```      \n",
    "If this isn't empty you already had docker running with a container. If you are not able to run the `docker` or `docker ps` commands from your command line, **STOP**. You need to get this installed before continuing.\n",
    "\n",
    "### Installing the Core NLP container\n",
    "This part is pretty easy. You just need to run the following command at your command line:\n",
    "```\n",
    "docker run -p 9000:9000 --name coreNLP --rm -i -t motiz88/corenlp\n",
    "```\n",
    "This will pull motiz88's Docker port of Core NLP and run it using port 9000. This means that port 9000 from the container will be forwarded to port 9000 on your localhost (your computer). So, you can access the Core NLP API over `http://localhost:9000`. Note that this is a fairly large container so it may take a few minutes to download and install.\n",
    "\n",
    "To make sure that the server is running, in your browser go to http://localhost:9000. You should see:\n",
    "![Stanford Core NLP Server](core_nlp_server.png \"Core NLP Server\")\n",
    "\n",
    "If you don't, don't move forward until you can verify the Core NLP server is running. You might try `docker ps` to see if the container is listed. If it is, you can scope out the logs with `docker logs coreNLP`. If it *is* running feel free to play around with the server UI. Input some text to get a feel for how it works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Extraction with Core NLP Server\n",
    "To use Core NLP Server, we are going to leverage the `pycorenlp` Python wrapper which can be installed with `pip install pycorenlp`. Once that's installed, you can instantiate a connection with the coreNLP server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's take a look at the basic functionality by feeding a few sentences of text to the coreNLP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output object has keys: dict_keys(['sentences'])\n",
      "Each sentence object has keys: dict_keys(['index', 'parse', 'tokens'])\n"
     ]
    }
   ],
   "source": [
    "text = (\"Bill and Ted are excellent! \"\n",
    "        \"Pusheen Smith and Jillian Marie walked along the beach; Pusheen led the way. \"\n",
    "        \"Pusheen wanted to surf, but fell off the surfboard. \"\n",
    "        \"They are both friends with Jean Claude van Dam, Sam's neighbor.\")\n",
    "output = nlp.annotate(text, properties={\n",
    "  'annotators': 'ner',\n",
    "  'outputFormat': 'json'\n",
    "  })\n",
    "\n",
    "print('The output object has keys: {}'.format(output.keys()))\n",
    "print('Each sentence object has keys: {}'.format(output['sentences'][0].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `output` object, as you can see for yourself is extremely verbose. It's comprised of a top-level `key` called `sentences` which contains one object per sentence. Each `sentence` object has an array of `token` objects that can be accessed at `output['sentences'][i]['tokens']` where `i` is the index (e.g. 0, 1, 2, etc) of the sentence of interest.\n",
    "\n",
    "What is a `token` you ask? Typically in natural language processing (NLP) when you process text you want to `tokenize` it. This means splitting the text into its respective components at the word and punctuation level. So, the sentence `'The quick brown fox jumped over the lazy dog.'` would be tokenized into an array that looks like:\n",
    "`['The','quick','brown','fox','jumped','over','the','lazy','dog','.']`. Some tokenizers ignore punctuation; others retain it.\n",
    "\n",
    "You can print out the `output` if you're interested in seeing what it looks like. That said, we need to be able to identify the people that the `ner` or Named Entity Recognition module discovered. So, let's go ahead and define a function which takes a set of sentence tokens and finds the tokens which were labeled as `PERSON`. This gets a little tricky as individual tokens can be labeled as `PERSON` when they actually correspond to the same person. For example, the tokens `Jean`, `Claude`, `van`, and `Dam` all correspond to the same person. So, the function below take `tokens` which are contiguous (next to one another) within the same sentence and combines them into the same person entity. Perfect!\n",
    "\n",
    "*By the way, this `proc_sentence` function is not very Pythonic. Ideas for doing this more efficiently are welcome!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_sentence(tokens):\n",
    "    \"\"\"\n",
    "    Takes as input a set of tokens from Stanford Core NLP output and returns \n",
    "    the set of peoplefound within the sentence. This relies on the fact that\n",
    "    named entities which are contiguous within a sentence should be part of \n",
    "    the same name. For example, in the following:\n",
    "    [\n",
    "        {'word': 'Brandon', 'ner': 'PERSON'},\n",
    "        {'word': 'Rose', 'ner': 'PERSON'},\n",
    "        {'word': 'eats', 'ner': 'O'},\n",
    "        {'word': 'bananas', 'ner': 'O'}\n",
    "    ]\n",
    "    we can safely assume that the contiguous PERSONs Brandon + Rose are part of the \n",
    "    same named entity, Brandon Rose.\n",
    "    \"\"\"\n",
    "    people = set()\n",
    "    token_count = 0\n",
    "    for i in range(len(tokens)):\n",
    "        if token_count < len(tokens):\n",
    "            person = ''\n",
    "            token = tokens[token_count]\n",
    "            if token['ner'] == 'PERSON':\n",
    "                person += token['word'].lower()\n",
    "                checking = True\n",
    "                while checking == True:\n",
    "                    if token_count + 1 < len(tokens):\n",
    "                        if tokens[token_count + 1]['ner'] == 'PERSON':\n",
    "                            token_count += 1\n",
    "                            person += ' {}'.format(tokens[token_count]['word'].lower())\n",
    "                        else:\n",
    "                            checking = False\n",
    "                            token_count += 1\n",
    "                    else:\n",
    "                        checking = False\n",
    "                        token_count += 1\n",
    "            else:\n",
    "                token_count += 1\n",
    "            if person != '':\n",
    "                people.add(person)\n",
    "    return people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the people which we can extract from each of the sentences. Note that the output of the `proc_sentence` function is a `set`, which means that it will only contain unique people entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ted', 'bill'}\n",
      "{'pusheen smith', 'pusheen', 'jillian marie'}\n",
      "{'pusheen'}\n",
      "{'sam', 'jean claude van dam'}\n"
     ]
    }
   ],
   "source": [
    "for sent in output['sentences']:\n",
    "    people = proc_sentence(sent['tokens'])\n",
    "    print(people)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we receive a `set` of the extracted people entities from each sentence. We can join the results with a superset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pusheen', 'bill', 'sam', 'jean claude van dam', 'jillian marie', 'ted', 'pusheen smith'}\n"
     ]
    }
   ],
   "source": [
    "people_super = set()\n",
    "for sent in output['sentences']:\n",
    "    people = proc_sentence(sent['tokens'])\n",
    "    for person in people:\n",
    "        people_super.add(person)\n",
    "\n",
    "print(people_super)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good, except notice that we see two items for Pusheen: `'pusheen'` and `'pusheen smith'`. We've done a decent job of entity extraction, but we need to take some additional steps for entity resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Resolution with Fuzzywuzzy\n",
    "\n",
    "If entity extraction is the process of finding entities (in this case, people) within a body of text then entity resolution is the process of putting like with like. As humans we know that `pusheen` and `pusheen smith` are the same person. How do we get a computer to do the same?\n",
    "\n",
    "There are many approaches that you can take for this, but we are going to use fuzzy deduplication found within a Python package called [fuzzywuzzy](https://github.com/seatgeek/fuzzywuzzy) (`pip install fuzzywuzzy`). Specifically, we'll use the fuzzy deduplication function (shameless plug, this is something I contributed to the fuzzywuzzy project). We can use the defaults, however you are welcome to tune the [parameters](https://github.com/seatgeek/fuzzywuzzy/blob/master/fuzzywuzzy/process.py#L167-L193).\n",
    "\n",
    "Note that you may be asked to optionally install `python-Levenshtein` to speed up `fuzzywuzzy`; you can do this with `pip install python-Levenshtein`.\n",
    "\n",
    "As an example of what fuzzy deduping is, let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy.process import dedupe as fuzzy_dedupe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our last step, we already have a list containing duplicates where some entities are partial representations of the other (`pusheen` vs. `pusheen smith`). Using fuzzywuzzy's dedupe function we can take care of this pretty easily. Fuzzywuzzy defaults to returning the longest representation of the resolved entity as it assumes this contains the most information. So, we expect to see `pusheen` resolve to `pusheen smith`. Also, fuzzywuzzy can handle slight mispellings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pusheen smith', 'bill', 'sam', 'jean claude van dam', 'jillian marie', 'ted'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contains_dupes = list(people_super)\n",
    "fuzzy_dedupe(contains_dupes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks like a useful list of entities to me!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting some data\n",
    "For this guide I'll be using a selection of news articles from Breitbart's Big Government section. Who knows, maybe we'll gain some insights into the networks at play in \"Big Government.\" Could be fun.\n",
    "\n",
    "To get the articles, I'm using [Newspaper](https://github.com/codelucas/newspaper). I'm going to scrape about 150 articles off the [Breitbart Big Government section](http://www.breitbart.com/big-government/).\n",
    "\n",
    "If you have your own data that's cool too. When you load the data it should be in JSON form:\n",
    "```\n",
    "{\n",
    "    0: {'article': 'some article text here'},\n",
    "    1: {'article': 'some other articles text here'},\n",
    "    ...\n",
    "    n: {'article': 'the nth articles text'}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import newspaper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to profile the site to find articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "breitbart = newspaper.build('http://www.breitbart.com/big-government/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can actually download them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained 10 articles\n",
      "Obtained 20 articles\n",
      "Obtained 30 articles\n",
      "Obtained 40 articles\n",
      "Obtained 50 articles\n",
      "Obtained 60 articles\n",
      "Obtained 70 articles\n",
      "Obtained 80 articles\n",
      "Obtained 90 articles\n",
      "Obtained 100 articles\n",
      "Obtained 110 articles\n",
      "Obtained 120 articles\n",
      "Obtained 130 articles\n",
      "Obtained 140 articles\n",
      "Obtained 150 articles\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "count = 0\n",
    "for article in breitbart.articles:\n",
    "    time.sleep(1)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    text = article.text\n",
    "    corpus.append(text)\n",
    "    if count % 10 == 0 and count != 0:\n",
    "        print('Obtained {} articles'.format(count))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this type of scraping can lead to your IP address getting flagged by some news sites, I've added a small sleep of 1 second between each article download. Just in case we get flagged, make sure to save our corpus. If you have a hard time using `newspaper` to get data you can just load up the data from `corpus.txt` within the github repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus.txt', 'a') as fp:\n",
    "    count = 0\n",
    "    for item in corpus:\n",
    "        loaded = item.encode('utf-8')\n",
    "        loaded_j = {count: loaded}\n",
    "        fp.write(json.dumps(loaded_j) + '\\n')\n",
    "        count += 1\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read back in the data we wrote to disk in the format of:\n",
    "```\n",
    "data[index]: {'article': 'article text'}\n",
    "```\n",
    "where the index is the order we read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "with open('corpus.txt', 'r') as fp:\n",
    "    for line in fp:\n",
    "        item = json.loads(line)\n",
    "        key = int(list(item.keys())[0])\n",
    "        value = list(item.values())[0].encode('ascii','ignore')\n",
    "        data[key] = {'article':value}\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the entities for each of the articles we've grabbed. We'll write the results back to the `data` dictionary in the format:\n",
    "```\n",
    "data[index]: {\n",
    "              'article': article text,\n",
    "              'people': [person entities]\n",
    "             }\n",
    "```\n",
    "\n",
    "Let's make a function that wraps up both using the Core NLP Server and Fuzzywuzzy to return the correct entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proc_article(article):\n",
    "    \"\"\"\n",
    "    Wrapper for coreNLP and fuzzywuzzy entity extraction and entity resolution.\n",
    "    \"\"\"\n",
    "    output = nlp.annotate(article, properties={\n",
    "      'annotators': 'ner',\n",
    "      'outputFormat': 'json'\n",
    "      })\n",
    "    \n",
    "    people_super = set()\n",
    "    for sent in output['sentences']:\n",
    "        people = proc_sentence(sent['tokens'])\n",
    "        for person in people:\n",
    "            people_super.add(person)\n",
    "\n",
    "    contains_dupes = list(people_super)\n",
    "    \n",
    "    deduped = fuzzy_dedupe(contains_dupes)\n",
    "    \n",
    "    return deduped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now process each article we downloaded. Note that sometimes `newspaper` will return an empty article so we can double check for these to make sure that we don't try to send them to Core NLP Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_keys = []\n",
    "for key in data:\n",
    "     # makes sure that the article actually has text\n",
    "    if data[key]['article'] != '':\n",
    "        people = proc_article(str(data[key]['article']))\n",
    "        data[key]['people'] = people\n",
    "    # if it's an empty article, let's save the key in `fail_keys`\n",
    "    else: \n",
    "        fail_keys.append(key)\n",
    "\n",
    "# now let's ditch any pesky empty articles\n",
    "for key in fail_keys:\n",
    "    data.pop(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to actually generate the network graph. I'll use the Python library `networkx` (`pip install networkx`) to build the network graph. To do this, I need to generate a dictionary of entities where each key is a unique entity and the values are a list of vertices that entity is connected to via an edge. For example, here we are indicating that George Clooney is connected to Bill Murray, Brad Pitt, and Seth Myers and has the highest degree centrality in the social network (due to having the highest number of edges).\n",
    "```\n",
    "{'George Clooney': ['Bill Murray', 'Brad Pitt', 'Seth Myers'],\n",
    " 'Bill Murray': ['Brad Pitt', 'George Clooney'],\n",
    " 'Seth Myers: ['George Clooney'],\n",
    " 'Brad Pitt': ['Bill Murray', 'George Clooney']\n",
    " '}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "from itertools import combinations\n",
    "from fuzzywuzzy.process import extractBests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Across document entity resolution\n",
    "Before we get started building our graph, we need to conduct entity resolution across our document corpus. We already did this at the document level, but surely different articles will refer to the President and others in different ways (e.g. \"Donald Trump\", \"Donald J. Trump\", \"President Trump\"). We can deal with this in the same way we handled differences within the same article with a slight addition: we need to build a lookup dictionary so that we can quickly convert the original entity into its resolved form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_lookup = {}\n",
    "for kk, vv in data.items():\n",
    "    for person in vv['people']:\n",
    "        person_lookup[person] = ''\n",
    "\n",
    "people_deduped = list(fuzzy_dedupe(person_lookup.keys()))\n",
    "\n",
    "# manually add the donald back in since fuzzy_dedupe will preference donald trump jr.\n",
    "people_deduped.append('donald trump')\n",
    "\n",
    "for person in person_lookup.keys():\n",
    "    match = extractBests(person, people_deduped)[0][0]\n",
    "    person_lookup[person] = match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donald trump resolves to: donald trump\n",
      "donald j. trump resolves to: donald trump\n",
      "donald trumps resolves to: donald trump\n"
     ]
    }
   ],
   "source": [
    "print('donald trump resolves to: {}'.format(person_lookup['donald trump']))\n",
    "print('donald j. trump resolves to: {}'.format(person_lookup['donald j. trump']))\n",
    "print('donald trumps resolves to: {}'.format(person_lookup['donald trumps']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! This way we don't have multiple entities in our graph representing the same person. Now we can go about building an adjacency dictionary which we'll call `entities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "entities = {}\n",
    "\n",
    "for key in data:\n",
    "    people = data[key]['people']\n",
    "    \n",
    "    doc_ents = []\n",
    "    for person in people:\n",
    "        # let's makes sure the person is a full name (has a space in between two words)\n",
    "        # let's also make sure that the total person name is at least 10 characters\n",
    "        if ' ' in person and len(person) > 10:\n",
    "            # note we will use our person_lookup to get the resolved person entity\n",
    "            doc_ents.append(person_lookup[person])\n",
    "    \n",
    "    for ent in doc_ents:\n",
    "        try:\n",
    "            entities[ent].extend([doc for doc in doc_ents if doc != ent])\n",
    "        except:\n",
    "            entities[ent] = [doc for doc in doc_ents if doc != ent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we need to actually build out the `networkx` graph. We can create a function which iteratively builds a networkx graph based on an entity adjacency dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def network_graph(ent_dict):\n",
    "    \"\"\"\n",
    "    Takes in an entity adjacency dictionary and returns a networkx graph\n",
    "    \"\"\"\n",
    "    index = ent_dict.keys()\n",
    "    \n",
    "    g = nx.Graph()\n",
    "\n",
    "    for ind in index:\n",
    "        ents = ent_dict[ind]\n",
    "\n",
    "        # Add previously unseen entities as nodes\n",
    "        for ent in ents:\n",
    "            if ent not in g:\n",
    "                g.add_node(ent, dict(\n",
    "                    name = ent,\n",
    "                    type = 'person',\n",
    "                    degree = str(len(ents))))\n",
    "\n",
    "    for ind in index:\n",
    "        ent = ent_dict[ind]\n",
    "        \n",
    "        for edge in ent:\n",
    "            if edge in index:\n",
    "                new_edge = (ind,edge)\n",
    "                if new_edge not in g.edges(): \n",
    "                    g.add_edge(ind, edge)\n",
    "        \n",
    "    js = json_graph.node_link_data(g)\n",
    "    js['adj'] = g.adj\n",
    "    return (g, js)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our function to build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = network_graph(entities)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue, we can do some cool things with our graph. One of them is determining who the most important people in our network are. We can quickly do this using [degree centrality](https://en.wikipedia.org/wiki/Centrality#Degree_centrality), which is a measure of the number of edges a node in our graph has. In this case, each node in the graph represents a person entity which was extracted from the Breitbart articles. The more people that a given individual co-occurred with the higher the degree of that node and the stronger his or her degree centrality.\n",
    "\n",
    "This image demonstrates how the degree of each node is calculated:\n",
    "\n",
    "<img src=\"degree_example.png\" alt=\"Degree Example\" style=\"display:inline\"/>\n",
    "\n",
    "When we calculated degree centrality with `networkx` we are returned normalized degree centrality scores which is the degree of a node divided by the maximum possible degree within the graph (`N-1`, where `N` is the number of nodes in the graph). Note that the term `node` and `vertex` can be taken to mean the same thing in network analysis. I prefer the term `node`.\n",
    "\n",
    "We can take a guess that since these articles were from Breitbart's government section there will be a number of articles referencing Donald Trump. So, we can assume he'll be at the top of the list. Who else will bubble up based on the number of people they are referenced along with? Let's find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donald trump: 0.5423728813559322\n",
      "vladimir -rsb- putin: 0.21468926553672316\n",
      "jerome hudson: 0.192090395480226\n",
      "george soros: 0.15254237288135594\n",
      "miosotis familias: 0.11864406779661017\n",
      "james baldwin: 0.10734463276836158\n",
      "barack obama: 0.10734463276836158\n",
      "patrisse cullors: 0.0847457627118644\n",
      "opal tometi: 0.0847457627118644\n",
      "micah x. johnson: 0.0847457627118644\n"
     ]
    }
   ],
   "source": [
    "centrality = nx.degree_centrality(graph)\n",
    "centrality_ = []\n",
    "for kk, vv in centrality.items():\n",
    "    centrality_.append((vv,kk))\n",
    "centrality_.sort(reverse=True)\n",
    "for person in centrality_[:10]:\n",
    "    print(\"{0}: {1}\".format(person[1],person[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact someone co-occurs in a document with another person does not mean anything specifically. We can't tell that they are friends, lovers, enemies, etc. However, when we do this type of analysis in aggregate we can begin to see patterns. For example, if Donald Trump and Vladimir Putin co-occur in a large number of documents we can assume that there is a dynamic or some sort of relationship between the two entities. There are actually approaches to entity extraction which attempt to explain the relationship between entities within documents, which might be fodder for another guide later on.\n",
    "\n",
    "All that said, typically this type of analysis requires a human in the loop (HITL) to validate the results since without additional context we can't tell exactly what to make of the fact that Donald Trump and Vladimir Putin appear to have a relationship within our graph's context.\n",
    "\n",
    "### Visualizing the graph\n",
    "\n",
    "With that caveat, we are about ready to visualize our graph. Since these types of graphs are best for exploration as interactives, we are going to rely on some javascript and HTML to render the graph. You'll need to ensure that you copy the `force` directory from the github repo for this project so that you have access to the correct `.css`, `.html` and `.js` files to build the charts.\n",
    "\n",
    "You actually can generate static network plots using networkx and matplotlib, but they aren't very fun and are hard to read. So, I've packaged up some [d3.js](https://d3js.org/) for you that will render the plots within an iframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.readwrite import json_graph\n",
    "\n",
    "for node in graph.nodes():\n",
    "    # let's drop any node that has a degree less than 13\n",
    "    # this is somewhat arbitrary, but helps trim our graph so that we\n",
    "    # only focus on relatively high degree entities\n",
    "    if int(graph.node[node]['degree']) < 13:\n",
    "        graph.remove_node(node)\n",
    "        \n",
    "d = json_graph.node_link_data(graph) # node-link format to serialize\n",
    "\n",
    "json.dump(d, open('force/force.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendering the chart\n",
    "Here's what's cool: we're going to embed an iframe within the notebook. However, if you want you've also got the d3.js based javascript and HTML code to pop this into your own website. I've done a couple customizations to this network diagram, including adding a tooltip with the entity name when you hover over the node. Also, the nodes are sticky--you can click the node and freeze it wherever you would like. This can seriously help when you are trying to understand what you are looking at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe height=400px width=100% src='force/force.html'></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe height=400px width=100% src='force/force.html'></iframe>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
